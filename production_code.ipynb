{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "430dea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "255a7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class clindata_downloader:\n",
    "    def __init__(self, base_url, params, outpath):\n",
    "        self.base_url = base_url   # Specify the API Base URL for ClinicalTrials.gov API\n",
    "        self.params = params        # Specify the query parameters\n",
    "        self.outpath = outpath\n",
    "        self.data_path = None\n",
    "        self.combined_studies = None\n",
    "        \n",
    "    def fetch_data(self):\n",
    "        # Initialize an empty list to store the data\n",
    "        data_list = []\n",
    "\n",
    "        while True:\n",
    "            if next_page_token:\n",
    "                # Add the nextPageToken to the parameters for subsequent requests\n",
    "                params[\"pageToken\"] = next_page_token\n",
    "\n",
    "            # Sending the request\n",
    "            response = requests.get(self.base_url, params=self.params)\n",
    "\n",
    "            # Handling the response\n",
    "            if response.status_code == 200:\n",
    "                # Parse the JSON response\n",
    "                data = response.json()\n",
    "                studies = data.get('studies', [])  # Extract the list of studies\n",
    "\n",
    "                data_path = os.path.join(outpath, \"rawdata\")\n",
    "                \n",
    "                # path where all the downloaded data from API will be saved in a folder named as rawdata\n",
    "                os.makedirs(self.outpath, exist_ok=True)\n",
    "                # Append studies to the data list\n",
    "                \n",
    "                data_list.extend(studies)\n",
    "\n",
    "                page_filename = f\"studies_page_{next_page_token}.json\" if next_page_token else \"studies_page_1.json\"\n",
    "                # dump to a json file\n",
    "\n",
    "                with open(os.path.join(data_path, page_filename), \"w\") as file:\n",
    "                    json.dump(data, file, indent=4)\n",
    "\n",
    "            #   Check for the nextPageToken\n",
    "                next_page_token = data.get(\"nextPageToken\")\n",
    "                print(next_page_token)\n",
    "\n",
    "                # Update the parameters with the nextPageToken\n",
    "                params[\"pageToken\"] = next_page_token\n",
    "\n",
    "                if not next_page_token:\n",
    "                    print(\"No more pages to fetch.\")\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print(f\"Error: {response.status_code} - {response.text}\")\n",
    "                break\n",
    "\n",
    "        # After all pages are fetched, save all data into a single file\n",
    "        with open(os.path.join(data_path, \"all_studies.json\"), \"w\") as file:\n",
    "            json.dump(data_list, file, indent=4)\n",
    "\n",
    "        print(f\"Fetched a total of {len(data_list)} studies.\")\n",
    "        self.data_path = data_path\n",
    "        return data_list\n",
    "    \n",
    "    def load_json(self):\n",
    "        with open(os.path.join(self.data_path, \"all_studies.json\")) as f:\n",
    "            combined_studies = json.load(f)\n",
    "            self.combined_studies = combined_studies\n",
    "            return self.combined_studies\n",
    "        \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "\n",
    "\n",
    "# Parameters for the query\n",
    "params = {\n",
    "    \"format\": \"json\",  # Requesting JSON format\n",
    "    \"query.term\": \"AREA[LastUpdatePostDate]RANGE[2024-10-20,2024-10-21]\",  # Essie expression\n",
    "}\n",
    "\n",
    "example_schema = {\n",
    " \"trialId\": \"NCT00560521\",\n",
    " \"title\": \"Effect of Continuous Positive Airway Pressure on Fluid Absorption Among Patients With Pleural Effusion Due to Tuberculosis\",\n",
    " \"startDate\": \"2005-03-01\",\n",
    " \"endDate\": \"2007-03-01\",\n",
    " \"phase\": \"Other\",\n",
    " \"principalInvestigator\": {\n",
    " \"name\": \"Juliana F Oliveira\",\n",
    " \"affiliation\": \"Universidade Federal do Rio de Janeiro\"\n",
    " },\n",
    " \"locations\": [\n",
    " {\n",
    " \"facility\": \"Federal University of Rio de Janeiro\",\n",
    " \"city\": \"Rio de Janeiro\",\n",
    " \"country\": \"Brazil\"\n",
    " }\n",
    " ],\n",
    " \"eligibilityCriteria\": \"Inclusion Criteria:\\n\\nConfirmed diagnosis of pleural tuberculosis.\\nPatients 18 years of age and older.\\n\\nExclusion criteria:\\n\\nBe under previous treatment of respirat ory physiotherapy.\\nIrregular use or abandonment of the anti-TB standard regimen.\\nTo fail one or more physiotherapy section.\\nTo fail one or more radiological evaluation.\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3cb599ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "    \n",
    "class clindata_parser:\n",
    "    def __init__(self, data_struct, schema, comb_studies, field_dict = None):\n",
    "        self.data_struct = data_struct\n",
    "        self.comb_studies = comb_studies\n",
    "        self.target_schema = schema\n",
    "        self.relevant_fields_df = None\n",
    "        \n",
    "        # Use the provided field_dict or set a default empty dictionary\n",
    "        self.field_dict = field_dict if field_dict is not None else {}\n",
    "        \n",
    "        self.super_dict = None\n",
    "        \n",
    "        \n",
    "    def xpath(a: str) -> str:\n",
    "        '''function to modify XPath strings.'''\n",
    "        if '/' in a:\n",
    "            b = a.replace('/', '.')\n",
    "            if 'Study' in b:\n",
    "                return b.replace('.Study.', '')\n",
    "        else:\n",
    "            return a\n",
    "\n",
    "    def map_datastruct(self):\n",
    "        relevant_fields = {}        # relevant fields that look like keys in example schema \n",
    "        for field in self.target_schema.keys():\n",
    "        #     print(field)\n",
    "            if field == 'trialId':\n",
    "                print(field)\n",
    "                field_original = field\n",
    "                field = 'id'\n",
    "                result = [y for y in self.data_struct['Piece Name'] if re.search(f\"{field}\", str(y), re.IGNORECASE)]\n",
    "                print(result)\n",
    "        #         print(self.data_struct[self.data_struct['Piece Name'].isin(result)][['Piece Name', 'Classic XPath']])\n",
    "                relevant_fields.update({field_original: self.data_struct[self.data_struct['Piece Name'].isin(result)][['Piece Name', 'Classic XPath']]})\n",
    "\n",
    "            elif field == 'endDate':\n",
    "                print(field)\n",
    "                field_original = field\n",
    "                field = 'CompletionDate'\n",
    "                result = [y for y in self.data_struct['Piece Name'] if re.search(f\"{field}\", str(y), re.IGNORECASE)]\n",
    "                print(result)\n",
    "                relevant_fields.update({field_original: self.data_struct[self.data_struct['Piece Name'].isin(result)][['Piece Name', 'Classic XPath']]})\n",
    "\n",
    "            elif field == 'principalInvestigator':\n",
    "                print(field)\n",
    "                field_original = field\n",
    "                field = 'Investigator'\n",
    "                result = [y for y in self.data_struct['Piece Name'] if re.search(f\"{field}\", str(y), re.IGNORECASE)]\n",
    "                print(result)\n",
    "                relevant_fields.update({field_original: self.data_struct[self.data_struct['Piece Name'].isin(result)][['Piece Name', 'Classic XPath']]})\n",
    "    \n",
    "    \n",
    "            else:\n",
    "                print(field)\n",
    "                result = [y for y in self.data_struct['Piece Name'] if re.search(f\"{field}\", str(y), re.IGNORECASE)]\n",
    "                print(result)\n",
    "                relevant_fields.update({field: self.data_struct[self.data_struct['Piece Name'].isin(result)][['Piece Name', 'Classic XPath']]})\n",
    "            print()\n",
    "    \n",
    "        relevant_fields_df = pd.concat(relevant_fields.values())\n",
    "        self.relevant_fields_df = relevant_fields_df\n",
    "        #     relevant_fields_df[relevant_fields_df['Piece Name'] == 'completionDateStruct']\n",
    "        return self.relevant_fields_df\n",
    "\n",
    "\n",
    "    def flatten_json(y, parent_key='', sep='.'):\n",
    "        \"\"\"\n",
    "        Flattens a nested JSON object into a single level.\n",
    "        Keys will be in the form 'parent.child.grandchild'.\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        for k, v in y.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "            if isinstance(v, dict):\n",
    "                items.extend(flatten_json(v, new_key, sep=sep).items())\n",
    "            elif isinstance(v, list):\n",
    "                for i, item in enumerate(v):\n",
    "                    items.extend(flatten_json({f\"{k}[{i}]\": item}, parent_key, sep=sep).items())\n",
    "            else:\n",
    "                items.append((new_key, v))\n",
    "        return dict(items)\n",
    "    \n",
    "    \n",
    "    def map_schema(self):\n",
    "        super_dict = {}\n",
    "        for n, x in enumerate(self.comb_studies):\n",
    "        #     print(n)\n",
    "            mapped_data = {}\n",
    "            flat_json_1 = flatten_json(self.comb_studies[n])\n",
    "        #     print(flat_json_1.keys())\n",
    "            for item in self.field_dict:\n",
    "                i = self.field_dict[item]\n",
    "        #         print(item, i, '__________________')\n",
    "\n",
    "                if isinstance(i, list):\n",
    "        #             print(i)\n",
    "                    try:\n",
    "                        investigater = {}\n",
    "                        for f in i:\n",
    "                            col_a = xpath(self.relevant_fields_df[self.relevant_fields_df['Piece Name'] == f]['Classic XPath'].values[0])\n",
    "        #                     print(col_a)\n",
    "                            try:\n",
    "                                investigater.update({f: flat_json_1[col_a]})\n",
    "                            except:\n",
    "                                pass\n",
    "                        mapped_data.update({item : investigater})\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    if item == 'locations':\n",
    "                        col_a = xpath(self.relevant_fields_df[self.relevant_fields_df['Piece Name'] == i]['Classic XPath'].values[0])\n",
    "        #                 print(col_a)\n",
    "\n",
    "                        try:\n",
    "                            location = [a for a in flat_json_1.keys() if a.startswith(str(col_a))]\n",
    "                            if len(location) == 1:\n",
    "                                mapped_data.update({item : flat_json_1[col_a]})\n",
    "                            else:\n",
    "                #             print(location)\n",
    "                                location_data = pd.Series({v: flat_json_1[v] for v in location})\n",
    "                    #             print(location_data)\n",
    "                                location_df = pd.DataFrame([range(1, len(location_data.index)+1), location_data, location_data.index]).T\n",
    "\n",
    "                                location_df['keyid'] = [re.findall(r'\\d+', string)[0] for string in location_df[2]]\n",
    "                                location_df['subfield'] = [string.split('].')[1] for string in location_df[2]]\n",
    "                                loc = {}\n",
    "                    #             print(location_df)\n",
    "                                for num, (g, h) in enumerate(location_df.groupby(by='keyid')):\n",
    "                                    loc.update({num: {key: val for key, val in zip(h['subfield'], h[1])}})\n",
    "                #                     print()\n",
    "            #                     print(loc)\n",
    "                                mapped_data.update({item : loc})\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    elif item == 'startDate':\n",
    "                        col_a = xpath(self.relevant_fields_df[self.relevant_fields_df['Piece Name'] == self.field_dict[item]]['Classic XPath'].values[0])\n",
    "                        try:\n",
    "                            sdate = [a for a in flat_json_1.keys() if a.startswith(str(col_a))][0]\n",
    "                            mapped_data.update({item : flat_json_1[sdate]})\n",
    "        #                     print(sdate)\n",
    "                        except:\n",
    "                            pass\n",
    "        #                 \n",
    "                    elif item == 'endDate':\n",
    "                        col_a = xpath(self.relevant_fields_df[self.relevant_fields_df['Piece Name'] == self.field_dict[item]]['Classic XPath'].values[0])\n",
    "                        try:\n",
    "                            edate = [a for a in flat_json_1.keys() if a.startswith(str(col_a))][0]\n",
    "                            mapped_data.update({item : flat_json_1[edate]})\n",
    "        #                     print(edate)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    else:\n",
    "        #                 print(i)\n",
    "                        try:\n",
    "                            col_a = xpath(self.relevant_fields_df[self.relevant_fields_df['Piece Name'] == self.field_dict[item]]['Classic XPath'].values[0])\n",
    "        #                     print(col_a, '***')\n",
    "        #                     print()\n",
    "                            mapped_data.update({item : flat_json_1[col_a]})\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            super_dict.update({n : mapped_data})\n",
    "      \n",
    "        self.super_dict = super_dict\n",
    "        return self.super_dict\n",
    "    \n",
    "    # Function to clean text in criteria\n",
    "    def clean_criteria(section):\n",
    "        # Remove bullet points (e.g., '* ', '1. ', etc.), colons, and split into lines\n",
    "        lines = re.split(r'\\n+', section)\n",
    "        cleaned_lines = [re.sub(r'^[\\*\\d\\.\\s]+|[:]', '', line).strip() for line in lines]\n",
    "        # Filter out empty strings\n",
    "        return [line for line in cleaned_lines if line]\n",
    "    \n",
    "    \n",
    "    def inclusion_criteria(self):\n",
    "        processed_data = {}\n",
    "        \n",
    "        # Initialize an empty list to collect error messages\n",
    "        error_log = []\n",
    "        \n",
    "        for i, j in list(self.super_dict.items()):\n",
    "        #     print(i, j.keys())\n",
    "        #     print(j['eligibilityCriteria'])\n",
    "            try:\n",
    "                criteria_text = j['eligibilityCriteria']\n",
    "        #         print(criteria_text)\n",
    "                # Splitting the text into inclusion and exclusion sections\n",
    "                sections = criteria_text.split(\"Exclusion Criteria:\")\n",
    "                inclusion_section = [y.split(\"Inclusion Criteria\")[1].strip('\\n') for y in [x for x in sections if x.startswith(\"Inclusion Criteria\")]]\n",
    "#                 print(inclusion_section)\n",
    "                inclusion_criteria = clean_criteria(\"\\n\".join(inclusion_section))\n",
    "#                 print(inclusion_criteria)\n",
    "                processed_data.update({i: inclusion_criteria})\n",
    "            except Exception as e:\n",
    "                error_log.append(f\"Error encountered with value i={i}: {e}\")\n",
    "                pass\n",
    "\n",
    "        # Write all collected errors to a log file at the end of the loop\n",
    "        if error_log:\n",
    "            with open(\"error_log.txt\", \"w\") as log_file:\n",
    "                log_file.write(\"\\n\".join(error_log))\n",
    "            print(\"Errors have been logged to 'error_log.txt'.\")\n",
    "        else:\n",
    "            print(\"No errors encountered during the loop.\")\n",
    "            \n",
    "            # Convert dictionary to DataFrame\n",
    "        df = pd.DataFrame.from_dict(processed_data, orient=\"index\").reset_index()\n",
    "        df.columns = [\"ID\"] + [f\"Sentence_{i}\" for i in range(1, len(df.columns))]\n",
    "\n",
    "        # df.columns = [\"ID\", \"Sentence\"]\n",
    "\n",
    "        df['merged'] = df.apply(lambda row: ' '.join([str(val) for val in row if val not in [None, '']]), axis=1)\n",
    "\n",
    "        print(df)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "## Manually written dict based on the keywords found from searching keys from example schema, required as the field names are different.\n",
    "# field_list = {'trialId':'nctId', 'title':'officialTitle', 'startDate':'startDateStruct', 'endDate':'completionDateStruct', 'phase':'Phase', 'principalInvestigator':['investigatorFullName',  'investigatorAffiliation'], 'locations':'locations\\xa0⤷', 'eligibilityCriteria':'eligibilityCriteria'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7b92ae4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Piece Name        Unnamed: 1 Unnamed: 2 Unnamed: 3  \\\n",
      "0             protocolSection  Protocol Section        NaN     STRUCT   \n",
      "1             ProtocolSection               NaN        NaN        NaN   \n",
      "2        identificationModule               NaN        NaN     STRUCT   \n",
      "3        IdentificationModule               NaN        NaN        NaN   \n",
      "4                       nctId               NaN     NCT-ID       TEXT   \n",
      "..                        ...               ...        ...        ...   \n",
      "467       IPDSharingTimeFrame               NaN        NaN        NaN   \n",
      "468            accessCriteria               NaN        NaN     MARKUP   \n",
      "469  IPDSharingAccessCriteria               NaN        NaN        NaN   \n",
      "470                       url               NaN        NaN       TEXT   \n",
      "471             IPDSharingURL               NaN        NaN        NaN   \n",
      "\n",
      "               Unnamed: 4                   Unnamed: 5 Unnamed: 6 Unnamed: 7  \\\n",
      "0         ProtocolSection               Study Protocol        NaN        NaN   \n",
      "1                     NaN                          NaN        NaN        NaN   \n",
      "2    IdentificationModule         Study Identification        NaN        NaN   \n",
      "3                     NaN                          NaN        NaN        NaN   \n",
      "4            text (stats)                          NaN        NaN        NaN   \n",
      "..                    ...                          ...        ...        ...   \n",
      "467                   NaN                          NaN        NaN        NaN   \n",
      "468      markup ✓ (stats)  IPD Sharing Access Criteria        NaN        NaN   \n",
      "469                   NaN                          NaN        NaN        NaN   \n",
      "470          text (stats)              IPD Sharing URL        NaN        NaN   \n",
      "471                   NaN                          NaN        NaN        NaN   \n",
      "\n",
      "                                         Classic XPath  \n",
      "0                                      protocolSection  \n",
      "1                               /Study/ProtocolSection  \n",
      "2                 protocolSection.identificationModule  \n",
      "3          /Study/ProtocolSection/IdentificationModule  \n",
      "4           protocolSection.identificationModule.nctId  \n",
      "..                                                 ...  \n",
      "467  /Study/ProtocolSection/IPDSharingStatementModu...  \n",
      "468  protocolSection.ipdSharingStatementModule.acce...  \n",
      "469  /Study/ProtocolSection/IPDSharingStatementModu...  \n",
      "470      protocolSection.ipdSharingStatementModule.url  \n",
      "471  /Study/ProtocolSection/IPDSharingStatementModu...  \n",
      "\n",
      "[472 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "data_structure = pd.read_csv(os.getcwd()+'\\\\datastruct\\\\Protocolsection.csv', header=0, skiprows=1)\n",
    "print(data_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1bb19051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "field_list = {'trialId':'nctId', 'title':'officialTitle', 'startDate':'startDateStruct', 'endDate':'completionDateStruct', 'phase':'Phase', 'principalInvestigator':['investigatorFullName',  'investigatorAffiliation'], 'locations':'locations\\xa0⤷', 'eligibilityCriteria':'eligibilityCriteria'}\n",
    "parse = clindata_parser(data_structure, example_schema, combined_studies, field_dict=field_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0292114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trialId\n",
      "['identificationModule', 'IdentificationModule', 'nctId', 'NCTId', 'nctIdAliases', 'NCTIdAlias', 'orgStudyIdInfo', 'OrgStudyIdInfo', 'id', 'OrgStudyId', 'OrgStudyIdType', 'OrgStudyIdLink', 'secondaryIdInfos', 'SecondaryIdInfo', 'id', 'SecondaryId', 'SecondaryIdType', 'SecondaryIdDomain', 'SecondaryIdLink', 'numSecondaryIds\\xa0✗', 'NumSecondaryIds', 'nctId', 'ExpandedAccessNCTId', 'statusForNctId', 'ExpandedAccessStatusForNCTId', 'nPtrsToThisExpAccNctId', 'NPtrsToThisExpAccNCTId', 'individual', 'ExpAccTypeIndividual', 'pmid', 'ReferencePMID', 'pmid', 'RetractionPMID', 'id', 'AvailIPDId']\n",
      "\n",
      "title\n",
      "['briefTitle', 'BriefTitle', 'officialTitle', 'OfficialTitle', 'investigatorTitle', 'ResponsiblePartyInvestigatorTitle', 'oldNameTitle', 'ResponsiblePartyOldNameTitle']\n",
      "\n",
      "startDate\n",
      "['startDateStruct', 'StartDateStruct', 'StartDate', 'StartDateType']\n",
      "\n",
      "endDate\n",
      "['primaryCompletionDateStruct', 'PrimaryCompletionDateStruct', 'PrimaryCompletionDate', 'PrimaryCompletionDateType', 'completionDateStruct', 'CompletionDateStruct', 'CompletionDate', 'CompletionDateType']\n",
      "\n",
      "phase\n",
      "['phases', 'Phase', 'numPhases\\xa0✗', 'NumPhases']\n",
      "\n",
      "principalInvestigator\n",
      "['investigatorFullName', 'ResponsiblePartyInvestigatorFullName', 'investigatorTitle', 'ResponsiblePartyInvestigatorTitle', 'investigatorAffiliation', 'ResponsiblePartyInvestigatorAffiliation']\n",
      "\n",
      "locations\n",
      "['contactsLocationsModule', 'ContactsLocationsModule', 'locations\\xa0⤷', 'LocationStatus', 'LocationState', 'numLocations\\xa0✗', 'NumLocations']\n",
      "\n",
      "eligibilityCriteria\n",
      "['eligibilityCriteria', 'EligibilityCriteria']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-7ad926e6172f>:123: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  location_data = pd.Series({v: flat_json_1[v] for v in location})\n"
     ]
    }
   ],
   "source": [
    "r_fields = parse.map_datastruct()\n",
    "super_dict = parse.map_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70bcd0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = os.getcwd()+'\\\\rawdata\\\\'\n",
    "# with open(os.path.join(data_path, \"all_studies.json\")) as f:\n",
    "#     combined_studies = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f8032c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-72-e9a49ff8e2e2>:37: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  location_data = pd.Series({v: flat_json_1[v] for v in location})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c889c1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trialId', 'title', 'startDate', 'endDate', 'principalInvestigator', 'locations', 'eligibilityCriteria'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_dict[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a341e6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors have been logged to 'error_log.txt'.\n",
      "      ID                                         Sentence_1  \\\n",
      "0      0  Patient or legally authorized representative p...   \n",
      "1      1  Non-small cell lunch cancer (NSLC) with untrea...   \n",
      "2      2                                            Age≥18y   \n",
      "3      3  Clinical diagnosis of mild to severe OSA in th...   \n",
      "4      4  Has localized high-risk or very high-risk pros...   \n",
      "..   ...                                                ...   \n",
      "565  567  patients with planned extubation assigned to t...   \n",
      "566  568                                home resident older   \n",
      "567  569  Written informed consent and HIPAA authorizati...   \n",
      "568  570                       A patient age of 14-70 years   \n",
      "569  571  Patients with head and neck cancer scheduled f...   \n",
      "\n",
      "                                            Sentence_2  \\\n",
      "0    Patient has or is intended to receive or be tr...   \n",
      "1                NSLC lacks oncogenic driver mutations   \n",
      "2    Histologically or cytologically confirmed meta...   \n",
      "3                         Age between 20-65 years old.   \n",
      "4    Has ≥3 prostate biopsy cores with ≥50% tumor i...   \n",
      "..                                                 ...   \n",
      "565                          able to swallow the fluid   \n",
      "566                                healthy individuals   \n",
      "567             Age ≥ 18 years at the time of consent.   \n",
      "568  Patients diagnosed with Severe Aplastic Anemia...   \n",
      "569                 Competent to give informed consent   \n",
      "\n",
      "                                            Sentence_3  \\\n",
      "0    Patient within enrollment window relative to t...   \n",
      "1           Absence of new onset neurological symptoms   \n",
      "2    Progression after or cannot undergo standard t...   \n",
      "3                                                 None   \n",
      "4      Has PSA \\>4 ng/mL ≤28 days prior to enrollment.   \n",
      "..                                                 ...   \n",
      "565                   willing to participate the study   \n",
      "566       the participants were aged between 65 and 95   \n",
      "567              ECOG Performance Status of 0, 1 or 2.   \n",
      "568  Subjects (or their legally acceptable represen...   \n",
      "569               Ability to read and write in English   \n",
      "\n",
      "                                            Sentence_4  \\\n",
      "0                                                 None   \n",
      "1      Presence of fewer than ten intracranial lesions   \n",
      "2    Presence of at least 1 non-irradiated tumor le...   \n",
      "3                                                 None   \n",
      "4    Has no evidence of distant metastases based on...   \n",
      "..                                                 ...   \n",
      "565                                               None   \n",
      "566                                               None   \n",
      "567  Histologically confirmed diagnosis of classica...   \n",
      "568                                               None   \n",
      "569                                               None   \n",
      "\n",
      "                                            Sentence_5  \\\n",
      "0                                                 None   \n",
      "1       Each lesion measures three centimeters or less   \n",
      "2                                         ECOG= 0 or 1   \n",
      "3                                                 None   \n",
      "4    Is a candidate for radical prostatectomy, and ...   \n",
      "..                                                 ...   \n",
      "565                                               None   \n",
      "566                                               None   \n",
      "567  Presence of radiographically measurable diseas...   \n",
      "568                                               None   \n",
      "569                                               None   \n",
      "\n",
      "                                            Sentence_6  \\\n",
      "0                                                 None   \n",
      "1         Life expectancy of greater than three months   \n",
      "2                            Written informed consent.   \n",
      "3                                                 None   \n",
      "4    Has not received nor plans to receive neoadjuv...   \n",
      "..                                                 ...   \n",
      "565                                               None   \n",
      "566                                               None   \n",
      "567  Prior therapy with check-point inhibitors (niv...   \n",
      "568                                               None   \n",
      "569                                               None   \n",
      "\n",
      "                                            Sentence_7  \\\n",
      "0                                                 None   \n",
      "1                   Adequate organ and marrow function   \n",
      "2    For Phase B 68Ga-NYM096 should meet the imagin...   \n",
      "3                                                 None   \n",
      "4                     Has a life expectancy \\>5 years.   \n",
      "..                                                 ...   \n",
      "565                                               None   \n",
      "566                                               None   \n",
      "567  Failed at least 2 prior therapies including cy...   \n",
      "568                                               None   \n",
      "569                                               None   \n",
      "\n",
      "                                            Sentence_8  \\\n",
      "0                                                 None   \n",
      "1    Ability to understand and willingness to sign ...   \n",
      "2                                                 None   \n",
      "3                                                 None   \n",
      "4    Additional key eligibility criteria immediatel...   \n",
      "..                                                 ...   \n",
      "565                                               None   \n",
      "566                                               None   \n",
      "567  Prior cancer treatment must be completed at le...   \n",
      "568                                               None   \n",
      "569                                               None   \n",
      "\n",
      "                                            Sentence_9  ... Sentence_59  \\\n",
      "0                                                 None  ...        None   \n",
      "1                                                 None  ...        None   \n",
      "2                                                 None  ...        None   \n",
      "3                                                 None  ...        None   \n",
      "4    Stage \\>pT3a (tumor has extended outside of th...  ...        None   \n",
      "..                                                 ...  ...         ...   \n",
      "565                                               None  ...        None   \n",
      "566                                               None  ...        None   \n",
      "567                Absolute Neutrophil Count ≥ 1000/μL  ...        None   \n",
      "568                                               None  ...        None   \n",
      "569                                               None  ...        None   \n",
      "\n",
      "    Sentence_60 Sentence_61 Sentence_62 Sentence_63 Sentence_64 Sentence_65  \\\n",
      "0          None        None        None        None        None        None   \n",
      "1          None        None        None        None        None        None   \n",
      "2          None        None        None        None        None        None   \n",
      "3          None        None        None        None        None        None   \n",
      "4          None        None        None        None        None        None   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "565        None        None        None        None        None        None   \n",
      "566        None        None        None        None        None        None   \n",
      "567        None        None        None        None        None        None   \n",
      "568        None        None        None        None        None        None   \n",
      "569        None        None        None        None        None        None   \n",
      "\n",
      "    Sentence_66 Sentence_67                                             merged  \n",
      "0          None        None  0 Patient or legally authorized representative...  \n",
      "1          None        None  1 Non-small cell lunch cancer (NSLC) with untr...  \n",
      "2          None        None  2 Age≥18y Histologically or cytologically conf...  \n",
      "3          None        None  3 Clinical diagnosis of mild to severe OSA in ...  \n",
      "4          None        None  4 Has localized high-risk or very high-risk pr...  \n",
      "..          ...         ...                                                ...  \n",
      "565        None        None  567 patients with planned extubation assigned ...  \n",
      "566        None        None  568 home resident older healthy individuals th...  \n",
      "567        None        None  569 Written informed consent and HIPAA authori...  \n",
      "568        None        None  570 A patient age of 14-70 years Patients diag...  \n",
      "569        None        None  571 Patients with head and neck cancer schedul...  \n",
      "\n",
      "[570 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "incl_data = parse.inclusion_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f353cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n"
     ]
    }
   ],
   "source": [
    "print(len(super_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620e542c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastruct_protocol = pd.read_csv(os.getcwd()+'\\\\datastruct\\\\Protocolsection.csv', header=0, skiprows=1)\n",
    "# print(datastruct_protocol)\n",
    "\n",
    "# f.close()\n",
    "data_path = os.getcwd()+'\\\\rawdata\\\\'\n",
    "with open(os.path.join(data_path, \"all_studies.json\")) as f:\n",
    "    combined_studies = json.load(f)\n",
    "#     print(combined_studies)\n",
    "len(combined_studies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ad63eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_schema = {\n",
    " \"trialId\": \"NCT00560521\",\n",
    " \"title\": \"Effect of Continuous Positive Airway Pressure on Fluid Absorption Among Patients With Pleural Effusion Due to Tuberculosis\",\n",
    " \"startDate\": \"2005-03-01\",\n",
    " \"endDate\": \"2007-03-01\",\n",
    " \"phase\": \"Other\",\n",
    " \"principalInvestigator\": {\n",
    " \"name\": \"Juliana F Oliveira\",\n",
    " \"affiliation\": \"Universidade Federal do Rio de Janeiro\"\n",
    " },\n",
    " \"locations\": [\n",
    " {\n",
    " \"facility\": \"Federal University of Rio de Janeiro\",\n",
    " \"city\": \"Rio de Janeiro\",\n",
    " \"country\": \"Brazil\"\n",
    " }\n",
    " ],\n",
    " \"eligibilityCriteria\": \"Inclusion Criteria:\\n\\nConfirmed diagnosis of pleural tuberculosis.\\nPatients 18 years of age and older.\\n\\nExclusion criteria:\\n\\nBe under previous treatment of respirat ory physiotherapy.\\nIrregular use or abandonment of the anti-TB standard regimen.\\nTo fail one or more physiotherapy section.\\nTo fail one or more radiological evaluation.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca55b204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trialId\n",
      "['identificationModule', 'IdentificationModule', 'nctId', 'NCTId', 'nctIdAliases', 'NCTIdAlias', 'orgStudyIdInfo', 'OrgStudyIdInfo', 'id', 'OrgStudyId', 'OrgStudyIdType', 'OrgStudyIdLink', 'secondaryIdInfos', 'SecondaryIdInfo', 'id', 'SecondaryId', 'SecondaryIdType', 'SecondaryIdDomain', 'SecondaryIdLink', 'numSecondaryIds\\xa0✗', 'NumSecondaryIds', 'nctId', 'ExpandedAccessNCTId', 'statusForNctId', 'ExpandedAccessStatusForNCTId', 'nPtrsToThisExpAccNctId', 'NPtrsToThisExpAccNCTId', 'individual', 'ExpAccTypeIndividual', 'pmid', 'ReferencePMID', 'pmid', 'RetractionPMID', 'id', 'AvailIPDId']\n",
      "\n",
      "title\n",
      "['briefTitle', 'BriefTitle', 'officialTitle', 'OfficialTitle', 'investigatorTitle', 'ResponsiblePartyInvestigatorTitle', 'oldNameTitle', 'ResponsiblePartyOldNameTitle']\n",
      "\n",
      "startDate\n",
      "['startDateStruct', 'StartDateStruct', 'StartDate', 'StartDateType']\n",
      "\n",
      "endDate\n",
      "['primaryCompletionDateStruct', 'PrimaryCompletionDateStruct', 'PrimaryCompletionDate', 'PrimaryCompletionDateType', 'completionDateStruct', 'CompletionDateStruct', 'CompletionDate', 'CompletionDateType']\n",
      "\n",
      "phase\n",
      "['phases', 'Phase', 'numPhases\\xa0✗', 'NumPhases']\n",
      "\n",
      "principalInvestigator\n",
      "['investigatorFullName', 'ResponsiblePartyInvestigatorFullName', 'investigatorTitle', 'ResponsiblePartyInvestigatorTitle', 'investigatorAffiliation', 'ResponsiblePartyInvestigatorAffiliation']\n",
      "\n",
      "locations\n",
      "['contactsLocationsModule', 'ContactsLocationsModule', 'locations\\xa0⤷', 'LocationStatus', 'LocationState', 'numLocations\\xa0✗', 'NumLocations']\n",
      "\n",
      "eligibilityCriteria\n",
      "['eligibilityCriteria', 'EligibilityCriteria']\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9491fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually written dict based on the keywords found from searching keys from example schema, required as the field names are different.\n",
    "field_list = {'trialId':'nctId', 'title':'officialTitle', 'startDate':'startDateStruct', 'endDate':'completionDateStruct', 'phase':'Phase', 'principalInvestigator':['investigatorFullName',  'investigatorAffiliation'], 'locations':'locations\\xa0⤷', 'eligibilityCriteria':'eligibilityCriteria'}\n",
    "\n",
    "\n",
    "def xpath(a: str) -> str:\n",
    "    '''function to modify XPath strings.'''\n",
    "    if '/' in a:\n",
    "        b = a.replace('/', '.')\n",
    "        if 'Study' in b:\n",
    "            return b.replace('.Study.', '')\n",
    "    else:\n",
    "        return a\n",
    "\n",
    "    \n",
    "def flatten_json(y, parent_key='', sep='.'):\n",
    "    \"\"\"\n",
    "    Flattens a nested JSON object into a single level.\n",
    "    Keys will be in the form 'parent.child.grandchild'.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in y.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_json(v, new_key, sep=sep).items())\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                items.extend(flatten_json({f\"{k}[{i}]\": item}, parent_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18f80798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "text = \"The patient has diabetes and hypertension.\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "# Output the results\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
